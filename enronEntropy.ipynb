{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnH/FUHInT0dGTIkjTUYOH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maReins/enronEntropy/blob/main/enronEntropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reins AI Entropy Calculation over the Enron Corpus\n",
        "\n",
        "Copyright (c) 2024 Reins AI, LLC\n",
        "\n",
        "This project is licensed under the MIT License. See the LICENSE file for details.\n",
        "\n",
        "## MIT License\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
        "\n",
        "- The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
      ],
      "metadata": {
        "id": "vgbtu8xICjTz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnWBVUrpBMj6",
        "outputId": "1d2ec2ba-9f72-44da-8717-514be43f4ff3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Extracting files...\n"
          ]
        }
      ],
      "source": [
        "#Extract Enron dataset from tar in gdrive and convert into CSV for further processing. Change paths where indicated.\n",
        "import os\n",
        "import tarfile\n",
        "import email\n",
        "import pandas as pd\n",
        "from email.parser import Parser\n",
        "from tqdm import tqdm  # for progress bar\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "def extract_tar(tar_path, extract_path):\n",
        "    with tarfile.open(tar_path, 'r:gz') as tar:\n",
        "        print(\"Extracting files...\")\n",
        "        tar.extractall(path=extract_path)\n",
        "    print(\"Extraction complete.\")\n",
        "\n",
        "def parse_email(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    email_parser = Parser()\n",
        "    parsed_email = email_parser.parsestr(content)\n",
        "\n",
        "    return {\n",
        "        'subject': parsed_email['subject'],\n",
        "        'from': parsed_email['from'],\n",
        "        'to': parsed_email['to'],\n",
        "        'date': parsed_email['date'],\n",
        "        'body': get_email_body(parsed_email)\n",
        "    }\n",
        "\n",
        "def get_email_body(parsed_email):\n",
        "    if parsed_email.is_multipart():\n",
        "        return '\\n'.join(part.get_payload() for part in parsed_email.get_payload() if part.get_content_type() == 'text/plain')\n",
        "    else:\n",
        "        return parsed_email.get_payload()\n",
        "\n",
        "def process_enron_data(root_dir):\n",
        "    emails = []\n",
        "    for root, dirs, files in tqdm(os.walk(root_dir), desc=\"Processing emails\"):\n",
        "        for file in files:\n",
        "            if file.endswith('.'):  # Enron emails don't have file extensions\n",
        "                file_path = os.path.join(root, file)\n",
        "                try:\n",
        "                    email_data = parse_email(file_path)\n",
        "                    emails.append(email_data)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "    return pd.DataFrame(emails)\n",
        "\n",
        "# Paths\n",
        "#add your path\n",
        "tar_path = '/content/gdrive/My Drive/enron_mail_20150507.tar.gz'\n",
        "extract_path = '/content/gdrive/My Drive/enron_extracted'\n",
        "maildir_path = os.path.join(extract_path, 'maildir')\n",
        "\n",
        "# Extract the tar file\n",
        "extract_tar(tar_path, extract_path)\n",
        "\n",
        "# Process the extracted emails\n",
        "df = process_enron_data(maildir_path)\n",
        "\n",
        "# Now you can use this DataFrame for your analysis\n",
        "print(df.head())\n",
        "print(df.info())\n",
        "\n",
        "# Save to CSV if needed\n",
        "df.to_csv('/content/gdrive/My Drive/enron_emails.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Parse emails from csv file, clean, and create dataframe\n",
        "def parse_email(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    email_parser = Parser()\n",
        "    parsed_email = email_parser.parsestr(content)\n",
        "\n",
        "    return {\n",
        "        'subject': parsed_email['subject'],\n",
        "        'from': parsed_email['from'],\n",
        "        'to': parsed_email['to'],\n",
        "        'date': parsed_email['date'],\n",
        "        'body': get_email_body(parsed_email)\n",
        "    }\n",
        "\n",
        "def get_email_body(parsed_email):\n",
        "    if parsed_email.is_multipart():\n",
        "        return '\\n'.join(part.get_payload() for part in parsed_email.get_payload() if part.get_content_type() == 'text/plain')\n",
        "    else:\n",
        "        return parsed_email.get_payload()\n",
        "\n",
        "def process_enron_data(root_dir):\n",
        "    emails = []\n",
        "    for root, dirs, files in os.walk(root_dir):\n",
        "        for file in files:\n",
        "            if file.endswith('.'):  # Enron emails don't have file extensions\n",
        "                file_path = os.path.join(root, file)\n",
        "                try:\n",
        "                    email_data = parse_email(file_path)\n",
        "                    emails.append(email_data)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "    return pd.DataFrame(emails)\n",
        "\n",
        "# Usage\n",
        "root_dir = '/content/gdrive/My Drive/enron_extracted'  # Replace with your actual path\n",
        "df = process_enron_data(root_dir)\n",
        "\n",
        "# Now you can use this DataFrame for your analysis\n",
        "print(df.head())\n",
        "print(df.info())\n",
        "\n",
        "# Save to CSV if needed\n",
        "df.to_csv('/content/gdrive/My Drive/enron_emails_clean.csv', index=False)"
      ],
      "metadata": {
        "id": "8OespBgJlvu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compute entropy and mutual information on emails by month\n",
        "\n",
        "from scipy.stats import entropy\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "def load_enron_data(file_path):\n",
        "    # Load the Enron email dataset\n",
        "    # Assuming the dataset is a CSV with columns: date, sender, recipient, subject, body\n",
        "    df = pd.read_csv(file_path)\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    return df\n",
        "\n",
        "def calculate_entropy(text):\n",
        "    vectorizer = CountVectorizer().fit([text])\n",
        "    vector = vectorizer.transform([text])\n",
        "    freq = vector.toarray()[0]\n",
        "    freq_norm = freq / np.sum(freq)\n",
        "    return entropy(freq_norm)\n",
        "\n",
        "def calculate_mutual_information(text1, text2):\n",
        "    vectorizer = CountVectorizer().fit([text1, text2])\n",
        "    vector1 = vectorizer.transform([text1]).toarray()[0]\n",
        "    vector2 = vectorizer.transform([text2]).toarray()[0]\n",
        "\n",
        "    p1 = vector1 / np.sum(vector1)\n",
        "    p2 = vector2 / np.sum(vector2)\n",
        "\n",
        "    joint_p = np.outer(p1, p2)\n",
        "    mi = np.sum(joint_p * np.log2(joint_p / (p1[:, np.newaxis] * p2[np.newaxis, :])))\n",
        "    return mi\n",
        "\n",
        "def analyze_enron_data(df, start_year, end_year):\n",
        "    df_filtered = df[(df['date'].dt.year >= start_year) & (df['date'].dt.year <= end_year)]\n",
        "\n",
        "    monthly_entropy = defaultdict(list)\n",
        "    monthly_mi = defaultdict(list)\n",
        "\n",
        "    for name, group in df_filtered.groupby(pd.Grouper(key='date', freq='M')):\n",
        "        month_key = name.strftime('%Y-%m')\n",
        "\n",
        "        # Calculate average entropy for the month\n",
        "        entropies = group['body'].apply(calculate_entropy)\n",
        "        monthly_entropy[month_key] = entropies.mean()\n",
        "\n",
        "        # Calculate average mutual information for the month\n",
        "        if len(group) > 1:\n",
        "            mis = [calculate_mutual_information(group['body'].iloc[i], group['body'].iloc[i+1])\n",
        "                   for i in range(len(group)-1)]\n",
        "            monthly_mi[month_key] = np.mean(mis)\n",
        "        else:\n",
        "            monthly_mi[month_key] = 0\n",
        "\n",
        "    return monthly_entropy, monthly_mi\n",
        "\n",
        "def plot_metrics(monthly_entropy, monthly_mi):\n",
        "    months = list(monthly_entropy.keys())\n",
        "    entropy_values = list(monthly_entropy.values())\n",
        "    mi_values = list(monthly_mi.values())\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
        "\n",
        "    ax1.plot(months, entropy_values, marker='o')\n",
        "    ax1.set_title('Monthly Average Entropy')\n",
        "    ax1.set_xlabel('Month')\n",
        "    ax1.set_ylabel('Entropy')\n",
        "    ax1.set_xticklabels(months, rotation=45)\n",
        "\n",
        "    ax2.plot(months, mi_values, marker='o', color='r')\n",
        "    ax2.set_title('Monthly Average Mutual Information')\n",
        "    ax2.set_xlabel('Month')\n",
        "    ax2.set_ylabel('Mutual Information')\n",
        "    ax2.set_xticklabels(months, rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Main execution\n",
        "#change path to clean csv.\n",
        "file_path = '/content/gdrive/My Drive/enron_emails_clean.csv'  # Replace with your actual file path\n",
        "df = load_enron_data(file_path)\n",
        "\n",
        "start_year = 2000  # Replace with your desired start year\n",
        "end_year = 2002    # Replace with your desired end year\n",
        "\n",
        "monthly_entropy, monthly_mi = analyze_enron_data(df, start_year, end_year)\n",
        "plot_metrics(monthly_entropy, monthly_mi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "2goF2SfIiWxM",
        "outputId": "39ea7928-983e-4e62-f72b-33463195bb2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'path_to_your_enron_dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9f43f7a5c303>\u001b[0m in \u001b[0;36m<cell line: 81>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# Main execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'path_to_your_enron_dataset.csv'\u001b[0m  \u001b[0;31m# Replace with your actual file path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_enron_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0mstart_year\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m  \u001b[0;31m# Replace with your desired start year\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-9f43f7a5c303>\u001b[0m in \u001b[0;36mload_enron_data\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Load the Enron email dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Assuming the dataset is a CSV with columns: date, sender, recipient, subject, body\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1706\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    861\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    864\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_your_enron_dataset.csv'"
          ]
        }
      ]
    }
  ]
}