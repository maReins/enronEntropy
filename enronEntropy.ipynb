{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnH/FUHInT0dGTIkjTUYOH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maReins/enronEntropy/blob/main/enronEntropy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reins AI Entropy Calculation over the Enron Corpus\n",
        "\n",
        "Copyright (c) 2024 Reins AI, LLC\n",
        "\n",
        "This project is licensed under the MIT License. See the LICENSE file for details.\n",
        "\n",
        "## MIT License\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
        "\n",
        "- The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
      ],
      "metadata": {
        "id": "vgbtu8xICjTz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnWBVUrpBMj6"
      },
      "outputs": [],
      "source": [
        "#Extract Enron dataset from tar in gdrive and convert into CSV for further processing. Change paths where indicated.\n",
        "import os\n",
        "import tarfile\n",
        "import email\n",
        "import pandas as pd\n",
        "from email.parser import Parser\n",
        "from tqdm import tqdm  # for progress bar\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "def extract_tar(tar_path, extract_path):\n",
        "    with tarfile.open(tar_path, 'r:gz') as tar:\n",
        "        print(\"Extracting files...\")\n",
        "        tar.extractall(path=extract_path)\n",
        "    print(\"Extraction complete.\")\n",
        "\n",
        "def parse_email(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    email_parser = Parser()\n",
        "    parsed_email = email_parser.parsestr(content)\n",
        "\n",
        "    return {\n",
        "        'subject': parsed_email['subject'],\n",
        "        'from': parsed_email['from'],\n",
        "        'to': parsed_email['to'],\n",
        "        'date': parsed_email['date'],\n",
        "        'body': get_email_body(parsed_email)\n",
        "    }\n",
        "\n",
        "def get_email_body(parsed_email):\n",
        "    if parsed_email.is_multipart():\n",
        "        return '\\n'.join(part.get_payload() for part in parsed_email.get_payload() if part.get_content_type() == 'text/plain')\n",
        "    else:\n",
        "        return parsed_email.get_payload()\n",
        "\n",
        "def process_enron_data(root_dir):\n",
        "    emails = []\n",
        "    for root, dirs, files in tqdm(os.walk(root_dir), desc=\"Processing emails\"):\n",
        "        for file in files:\n",
        "            if file.endswith('.'):  # Enron emails don't have file extensions\n",
        "                file_path = os.path.join(root, file)\n",
        "                try:\n",
        "                    email_data = parse_email(file_path)\n",
        "                    emails.append(email_data)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "    return pd.DataFrame(emails)\n",
        "\n",
        "# Paths\n",
        "#add your path\n",
        "tar_path = '/content/gdrive/My Drive/enron_mail_20150507.tar.gz'\n",
        "extract_path = '/content/gdrive/My Drive/enron_extracted'\n",
        "maildir_path = os.path.join(extract_path, 'maildir')\n",
        "\n",
        "# Extract the tar file\n",
        "extract_tar(tar_path, extract_path)\n",
        "\n",
        "# Process the extracted emails\n",
        "df = process_enron_data(maildir_path)\n",
        "\n",
        "# Now you can use this DataFrame for your analysis\n",
        "print(df.head())\n",
        "print(df.info())\n",
        "\n",
        "# Save to CSV if needed\n",
        "df.to_csv('/content/gdrive/My Drive/enron_emails.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Parse emails from csv file, clean, and create dataframe\n",
        "def parse_email(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    email_parser = Parser()\n",
        "    parsed_email = email_parser.parsestr(content)\n",
        "\n",
        "    return {\n",
        "        'subject': parsed_email['subject'],\n",
        "        'from': parsed_email['from'],\n",
        "        'to': parsed_email['to'],\n",
        "        'date': parsed_email['date'],\n",
        "        'body': get_email_body(parsed_email)\n",
        "    }\n",
        "\n",
        "def get_email_body(parsed_email):\n",
        "    if parsed_email.is_multipart():\n",
        "        return '\\n'.join(part.get_payload() for part in parsed_email.get_payload() if part.get_content_type() == 'text/plain')\n",
        "    else:\n",
        "        return parsed_email.get_payload()\n",
        "\n",
        "def process_enron_data(root_dir):\n",
        "    emails = []\n",
        "    for root, dirs, files in os.walk(root_dir):\n",
        "        for file in files:\n",
        "            if file.endswith('.'):  # Enron emails don't have file extensions\n",
        "                file_path = os.path.join(root, file)\n",
        "                try:\n",
        "                    email_data = parse_email(file_path)\n",
        "                    emails.append(email_data)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "    return pd.DataFrame(emails)\n",
        "\n",
        "# Usage\n",
        "root_dir = '/content/gdrive/My Drive/enron_extracted'  # Replace with your actual path\n",
        "df = process_enron_data(root_dir)\n",
        "\n",
        "# Now you can use this DataFrame for your analysis\n",
        "print(df.head())\n",
        "print(df.info())\n",
        "\n",
        "# Save to CSV if needed\n",
        "df.to_csv('/content/gdrive/My Drive/enron_emails_clean.csv', index=False)"
      ],
      "metadata": {
        "id": "8OespBgJlvu_",
        "outputId": "48487192-b444-46d6-9dfc-95ce79ac885a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 0 entries\n",
            "Empty DataFrame\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#compute entropy and mutual information on emails by month\n",
        "\n",
        "from scipy.stats import entropy\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "\n",
        "def load_enron_data(file_path):\n",
        "    # Load the Enron email dataset\n",
        "    # Assuming the dataset is a CSV with columns: date, sender, recipient, subject, body\n",
        "    df = pd.read_csv(file_path)\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    return df\n",
        "\n",
        "def calculate_entropy(text):\n",
        "    vectorizer = CountVectorizer().fit([text])\n",
        "    vector = vectorizer.transform([text])\n",
        "    freq = vector.toarray()[0]\n",
        "    freq_norm = freq / np.sum(freq)\n",
        "    return entropy(freq_norm)\n",
        "\n",
        "def calculate_mutual_information(text1, text2):\n",
        "    vectorizer = CountVectorizer().fit([text1, text2])\n",
        "    vector1 = vectorizer.transform([text1]).toarray()[0]\n",
        "    vector2 = vectorizer.transform([text2]).toarray()[0]\n",
        "\n",
        "    p1 = vector1 / np.sum(vector1)\n",
        "    p2 = vector2 / np.sum(vector2)\n",
        "\n",
        "    joint_p = np.outer(p1, p2)\n",
        "    mi = np.sum(joint_p * np.log2(joint_p / (p1[:, np.newaxis] * p2[np.newaxis, :])))\n",
        "    return mi\n",
        "\n",
        "def analyze_enron_data(df, start_year, end_year):\n",
        "    df_filtered = df[(df['date'].dt.year >= start_year) & (df['date'].dt.year <= end_year)]\n",
        "\n",
        "    monthly_entropy = defaultdict(list)\n",
        "    monthly_mi = defaultdict(list)\n",
        "\n",
        "    for name, group in df_filtered.groupby(pd.Grouper(key='date', freq='M')):\n",
        "        month_key = name.strftime('%Y-%m')\n",
        "\n",
        "        # Calculate average entropy for the month\n",
        "        entropies = group['body'].apply(calculate_entropy)\n",
        "        monthly_entropy[month_key] = entropies.mean()\n",
        "\n",
        "        # Calculate average mutual information for the month\n",
        "        if len(group) > 1:\n",
        "            mis = [calculate_mutual_information(group['body'].iloc[i], group['body'].iloc[i+1])\n",
        "                   for i in range(len(group)-1)]\n",
        "            monthly_mi[month_key] = np.mean(mis)\n",
        "        else:\n",
        "            monthly_mi[month_key] = 0\n",
        "\n",
        "    return monthly_entropy, monthly_mi\n",
        "\n",
        "def plot_metrics(monthly_entropy, monthly_mi):\n",
        "    months = list(monthly_entropy.keys())\n",
        "    entropy_values = list(monthly_entropy.values())\n",
        "    mi_values = list(monthly_mi.values())\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10))\n",
        "\n",
        "    ax1.plot(months, entropy_values, marker='o')\n",
        "    ax1.set_title('Monthly Average Entropy')\n",
        "    ax1.set_xlabel('Month')\n",
        "    ax1.set_ylabel('Entropy')\n",
        "    ax1.set_xticklabels(months, rotation=45)\n",
        "\n",
        "    ax2.plot(months, mi_values, marker='o', color='r')\n",
        "    ax2.set_title('Monthly Average Mutual Information')\n",
        "    ax2.set_xlabel('Month')\n",
        "    ax2.set_ylabel('Mutual Information')\n",
        "    ax2.set_xticklabels(months, rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Main execution\n",
        "#change path to clean csv.\n",
        "file_path = '/content/gdrive/My Drive/enron_emails_clean.csv'  # Replace with your actual file path\n",
        "df = load_enron_data(file_path)\n",
        "\n",
        "start_year = 2000  # Replace with your desired start year\n",
        "end_year = 2002    # Replace with your desired end year\n",
        "\n",
        "monthly_entropy, monthly_mi = analyze_enron_data(df, start_year, end_year)\n",
        "plot_metrics(monthly_entropy, monthly_mi)"
      ],
      "metadata": {
        "id": "2goF2SfIiWxM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}